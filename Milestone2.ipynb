{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00dcbed",
   "metadata": {
    "id": "d00dcbed"
   },
   "source": [
    "## Importing librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eecee168-8a26-4c5d-b113-cf6ebde9ca33",
   "metadata": {
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1668793625812,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "eecee168-8a26-4c5d-b113-cf6ebde9ca33"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "import json\n",
    "import string\n",
    "import requests\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ozTrXFKy16qt",
   "metadata": {
    "executionInfo": {
     "elapsed": 130259,
     "status": "ok",
     "timestamp": 1668793615897,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "ozTrXFKy16qt"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect\n",
    "!pip install names-dataset\n",
    "!pip install bertopic\n",
    "!pip install Wikidata\n",
    "!pip install wordcloud\n",
    "!pip install pyLDAvis\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072a8423-4a51-406f-bb0c-5991da4a2737",
   "metadata": {
    "id": "072a8423-4a51-406f-bb0c-5991da4a2737"
   },
   "outputs": [],
   "source": [
    "#EDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from wikidata.client import Client\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "#Preprocessing\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from names_dataset import NameDataset\n",
    "from itertools import chain \n",
    "import seaborn as sns\n",
    "\n",
    "#Stats\n",
    "import scipy.stats as stats\n",
    "\n",
    "#LDA\n",
    "from gensim.models import Phrases\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "#WordCloud \n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "#Topic visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "#BERTopic\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3240ba52",
   "metadata": {
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1668793740379,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "3240ba52"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1ea528",
   "metadata": {
    "executionInfo": {
     "elapsed": 1974,
     "status": "ok",
     "timestamp": 1668793744961,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "2f1ea528"
   },
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "download('averaged_perceptron_tagger')\n",
    "download('wordnet')\n",
    "download('omw-1.4')\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198b3e5",
   "metadata": {
    "id": "5198b3e5",
    "tags": []
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabee6f",
   "metadata": {
    "id": "5dabee6f",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "[[1]](http://www.cs.cmu.edu/~ark/personas/) CMU Movie Summary Corpus webpage  \n",
    "[[2]](http://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf) _Learning Latent Personas of Film Characters_, David Bamman, Brendan O'Connor and Noah A. Smith, ACL 2013, Sofia, Bulgaria, August 2013 \\\n",
    "[[3]](https://www.nltk.org/) NLKT documentation \n",
    "\n",
    "# Table of contents\n",
    "### [1. Metadata](#1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**[1.1 The data](#1.1)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**[1.2 Cleaning the dictionaries](#1.2)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**[1.3 Duplicate values](#1.3)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**[1.4 Missing values](#1.4)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.4.1 Recovering Ethnicities](#1.4.1)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.4.2 Recovering missing Actor names ](#1.4.2)\n",
    "\n",
    "\n",
    "### [2. Plot Summaries and Metascores](#2)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**[2.1 Preprocessing](#2.1)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1.1 Missing plot summaries check](#2.1.1)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1.2 Cleaning](#2.1.2)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1.3 Tokenization](#2.1.3)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1.4 PoS tagging](#2.1.4)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1.5 Lemmatization](#2.1.5)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1.6 Regrouping tokens and removing stop words](#2.1.6)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1.7 Integration into movies dataset ](#2.1.7)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1.8 Preprocessing results](#2.1.8)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**[2.2 Importing Metascore](#2.2)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**[2.3 Initial analysis](#2.3)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.1 Plot summaries structure](#2.3.1)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.2 Most common tokens](#2.3.2)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.3 Movie genres clustering](#2.3.3)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.4 Words polarity](#2.3.4)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.5 Ratings preliminary visualization](#2.3.5)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.6 WordCloud](#2.3.6)\n",
    "\n",
    "### [3. Topic extraction](#3)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**[3.1 LDA](#3.1)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1 Data preparation](#3.1.1)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2 Implementation](#3.1.2)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.3 Model evaluation](#3.1.3)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.4 Resulting topics](#3.1.4)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.5 Topic Visualizations](#3.1.5)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.6 Assigning topics to movies](#3.1.6)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**[3.2 BERTopic](#3.2)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1 Implementation](#3.2.1)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.2 Model evaluation](#3.2.2)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.3 Resulting topics](#3.2.3)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.4 Topic Visualizations](#3.2.4)\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.5 Assigning topics to movies](#3.2.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db749e",
   "metadata": {
    "id": "e0db749e",
    "tags": []
   },
   "source": [
    "# 1. Metadata <a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392f814",
   "metadata": {
    "id": "9392f814",
    "tags": []
   },
   "source": [
    "## 1.1 The Data  <a id='1.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b4e47",
   "metadata": {
    "id": "dd8b4e47",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "`movie.metadata.tsv.gz` [3.4 M]\n",
    "\n",
    "\n",
    "Metadata for 81,741 movies, extracted from the Noverber 4, 2012 dump of Freebase. Tab-separated. The columns are:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie name\n",
    "4. Movie release date\n",
    "5. Movie box office revenue\n",
    "6. Movie runtime\n",
    "7. Movie languages (Freebase ID:name tuples)\n",
    "8. Movie countries (Freebase ID:name tuples)\n",
    "9. Movie genres (Freebase ID:name tuples)\n",
    "\n",
    "\n",
    "`character.metadata.tsv.gz` [14 M]\n",
    "\n",
    "Metadata for 450,669 characters aligned to the movies above, extracted from the November 4, 2012 dump of Freebase. Tab-separated. The columns are:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie release date\n",
    "4. Character name\n",
    "5. Actor date of birth\n",
    "6. Actor gender\n",
    "7. Actor height (in meters)\n",
    "8. Actor ethnicity (Freebase ID)\n",
    "9. Actor name\n",
    "10. Actor age at movie release\n",
    "11. Freebase character/actor map ID\n",
    "12. Freebase character ID\n",
    "13. Freebase actor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45a638",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "executionInfo": {
     "elapsed": 3459,
     "status": "ok",
     "timestamp": 1668793825713,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "df45a638",
    "outputId": "4a8928ff-ed61-4383-b02c-75f12051857c"
   },
   "outputs": [],
   "source": [
    "movies_path = \"data/movie.metadata.tsv\"\n",
    "characters_path = \"data/character.metadata.tsv\"\n",
    "\n",
    "# naming the columns with adequate names\n",
    "movies_column_names = [\"WikiMovieID\", \"FreeMovieID\", \"Title\", \"ReleaseDate\", \"Revenue\", \"Runtime\", \"Languages\", \"Countries\", \"Genres\"]\n",
    "characters_column_names = [\"WikiMovieID\", \"FreeMovieID\", \"ReleaseDate\", \"CharName\", \"ActorDOB\", \"ActorGender\", \"ActorHeight\", \"FreeEthnicityID\",\\\n",
    "                           \"ActorName\", \"ActorAgeRelease\", \"FreeMapID\", \"FreeCharID\", \"FreeActorID\"]\n",
    "\n",
    "movies = pd.read_csv(movies_path, sep='\\t', header=None, names=movies_column_names)\n",
    "characters = pd.read_csv(characters_path, sep='\\t', header=None, names=characters_column_names)\n",
    "\n",
    "display(movies.head())\n",
    "display(characters.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e43f4",
   "metadata": {
    "id": "851e43f4"
   },
   "source": [
    "#### Initial remarks :\n",
    "- `ReleaseDate`doesn't have a standard format.\n",
    "- `Languages`, `Countries`and `Genres`columns have their values in a dictionary of form : `{Freebase id : Actual name}`\n",
    "\n",
    "#### Dataset length\n",
    "We verify that the given dataset entries are indeed as much as we expect : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fecf3e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1668727311572,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "8fecf3e4",
    "outputId": "5bc458e1-c51d-4f5e-ceeb-b3b247cf8784"
   },
   "outputs": [],
   "source": [
    "n_mov = len(movies)\n",
    "n_char = len(characters)\n",
    "print('Number of rows in the movie.metadata dataset :', n_mov)\n",
    "print('Number of rows in the character.metadata dataset :', n_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e709333",
   "metadata": {
    "id": "8e709333",
    "tags": []
   },
   "source": [
    "## 1.2 Cleaning Dictionaries  <a id='1.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1660c",
   "metadata": {
    "id": "bba1660c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Columns `Languages`, `Countries`, `Genres` are dictionaries containing both the Freebase ID and the actual name, for each entry. We will keep only the names for the sake of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8d836",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1668793828797,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "90e8d836",
    "outputId": "4ffdbae3-1abd-469b-b8a4-159b58c49193"
   },
   "outputs": [],
   "source": [
    "def extract_values(column):\n",
    "    values = []\n",
    "    column=json.loads(column)\n",
    "    for key in column:\n",
    "        values.append(column[key])\n",
    "    return values\n",
    "\n",
    "movies.Languages = movies.Languages.apply(extract_values)\n",
    "movies.Countries = movies.Countries.apply(extract_values)\n",
    "movies.Genres = movies.Genres.apply(extract_values)\n",
    "\n",
    "display(movies[[\"Languages\", \"Countries\", \"Genres\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c8bbe",
   "metadata": {
    "id": "c83c8bbe",
    "tags": []
   },
   "source": [
    "## 1.3 Duplicate values  <a id='1.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b310970",
   "metadata": {
    "id": "5b310970",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Starting with the characters dataset we only care if a whole row is duplicated as there is no specific feature that we prohibit from happenning twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a8151",
   "metadata": {
    "id": "a99a8151",
    "outputId": "5be9aca0-1324-4c5f-9d69-69086c83a9fd"
   },
   "outputs": [],
   "source": [
    "print(\"Duplicated rows in characters : {}\".format(characters.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c0c8c",
   "metadata": {
    "id": "bc3c0c8c"
   },
   "source": [
    "As for the movies dataset we have to be more careful as we don't want the same movie appearing twice under different release date or freebase id for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a10531",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19349,
     "status": "ok",
     "timestamp": 1668696122621,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "59a10531",
    "outputId": "d17fd8ea-e36d-4533-813c-aabd39513de5"
   },
   "outputs": [],
   "source": [
    "print(\"Duplicated rows in :\")\n",
    "for column in movies.columns:\n",
    "    duplicated_rows = movies[column].duplicated().sum()\n",
    "    print(\"\\t{} : {} \".format(column, duplicated_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172f746",
   "metadata": {
    "id": "4172f746"
   },
   "source": [
    "The fact that `Title` has a duplicated rows is a bit concerning. We see however than `WikiMovieID` and movies `FreeMovieID` have only unique entries. This indicates us that movies can have the same title while being actually different movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f311c4",
   "metadata": {
    "id": "05f311c4",
    "tags": []
   },
   "source": [
    "## 1.4 Missing values  <a id='1.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58450f5",
   "metadata": {
    "id": "c58450f5"
   },
   "source": [
    "We want to see which columns have missing entries, and for those columns what is the percentage of missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dd6c4",
   "metadata": {
    "id": "d47dd6c4",
    "outputId": "e887ffa2-2a65-44df-9846-297c8501015e"
   },
   "outputs": [],
   "source": [
    "movies.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c3474",
   "metadata": {
    "id": "287c3474"
   },
   "source": [
    "For the movies dataset, only `ReleaseDate`, `Revenue` and `Runtime` have missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3ec2d",
   "metadata": {
    "id": "c1d3ec2d",
    "outputId": "24b0ccce-3672-4955-f7c0-7f9dbfcdd5f7"
   },
   "outputs": [],
   "source": [
    "mov_missing = movies[['ReleaseDate', 'Revenue', 'Runtime']].isna().sum()\n",
    "print('Percentage of missing entries in the movie dataset (%):')\n",
    "for x in range(len(mov_missing.values)):\n",
    "    print(\"   \",mov_missing.index[x], ':', round(100*mov_missing.values[x]/n_mov, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2b6a4",
   "metadata": {
    "id": "ebe2b6a4"
   },
   "source": [
    "90% of the revenues are non specified. We won't use this feature in our project.  \n",
    "8% of the release dates and 25% of the runtimes are missing, we can fill them if we find the correct ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56f5b0",
   "metadata": {
    "id": "7e56f5b0",
    "outputId": "440e58e2-d283-4c22-89ca-ea36c91ec9f2"
   },
   "outputs": [],
   "source": [
    "characters.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475c477",
   "metadata": {
    "id": "4475c477",
    "outputId": "eed0f13c-6ff6-4ecd-8ba3-0eb41e585bf1"
   },
   "outputs": [],
   "source": [
    "char_missing = characters[['ReleaseDate', 'CharName', 'ActorDOB', 'ActorGender', 'ActorHeight', 'FreeEthnicityID', 'ActorName',\n",
    "                           'ActorAgeRelease', 'FreeCharID', 'FreeActorID']].isna().sum()\n",
    "print('Percentage of missing entries in the characters dataset (%):')\n",
    "for x in range(len(char_missing.values)):\n",
    "    print(\"   \",char_missing.index[x], ':', round(100*char_missing.values[x]/n_char, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2480e8c",
   "metadata": {
    "id": "b2480e8c",
    "tags": []
   },
   "source": [
    "### 1.4.1 Recovering Ethnicities  <a id='1.4.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2af37c",
   "metadata": {
    "id": "ab2af37c"
   },
   "source": [
    "`Ethnicity` only has the ID of the Freebase database whose API is currently depreciated. To solve this problem we use the [Freebase/Wikidata Mappings](https://developers.google.com/freebase#freebase-wikidata-mappings) which as the name suggests maps the given Freebase Ids to WikiData.\n",
    "\n",
    "- Note : The data has been created based on the Wikidata-Dump of October 28, 2013, and contains only those links that have at least two common Wikipedia-Links and not a single disagreeing Wikipedia-Link. Since the movies/characters dataset were gathered during that time too there is no time conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca31ff8",
   "metadata": {
    "id": "0ca31ff8",
    "outputId": "27a4dd39-6205-41fe-b4f0-462ac5df51e5"
   },
   "outputs": [],
   "source": [
    "mappings_path = \"data/fb2w.nt.gz\"\n",
    "maps = pd.read_csv(mappings_path, sep='\\t', header=None, skiprows=4, names=[\"Freebase\", \"W3\", \"Wiki\"])\n",
    "display(maps.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105eeb95",
   "metadata": {
    "id": "105eeb95"
   },
   "source": [
    "We only need the ID and not the whole website link so we transform columns in the following way :\n",
    "- Freebase : <http://rdf.freebase.com/ns/m.CODE_ID> -> /m/CODE_ID\n",
    "- Wiki : <http://www.wikidata.org/entity/CODE_ID> -> CODE_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9b794",
   "metadata": {
    "id": "06c9b794"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transforms the Freebase website to Freebase ID\n",
    "\"\"\"\n",
    "def elim_freebase(web):\n",
    "    return \"/m/\"+web[30:-1]\n",
    "\n",
    "\"\"\"\n",
    "Transforms the Freebase website to Freebase ID\n",
    "\"\"\"\n",
    "def elim_wiki(web):\n",
    "    return web[32:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d3cff",
   "metadata": {
    "id": "ea9d3cff",
    "outputId": "8e719db7-fa30-49ec-b507-042452a1bd8e"
   },
   "outputs": [],
   "source": [
    "maps[\"free\"] = maps.Freebase.apply(elim_freebase)\n",
    "maps[\"wiki\"] = maps.Wiki.apply(elim_wiki)\n",
    "\n",
    "# We don't need the rest of the columns so we drop them \n",
    "maps.drop(columns = [\"Freebase\", \"W3\", \"Wiki\"], inplace=True)\n",
    "maps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1bab75",
   "metadata": {
    "id": "dd1bab75"
   },
   "source": [
    "Firstly, we get the Wikidata IDs based on the mapping we created :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c92e7",
   "metadata": {
    "id": "8f2c92e7",
    "outputId": "479584ce-5596-42db-e043-e471a6a4b450"
   },
   "outputs": [],
   "source": [
    "characters = characters.merge(maps, how=\"left\", left_on=\"FreeEthnicityID\", right_on=\"free\")\n",
    "characters.drop(columns=[\"free\"], inplace=True)\n",
    "characters.rename(columns={\"wiki\":\"Ethnicity_W\"}, inplace=True)\n",
    "display(characters.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5e438",
   "metadata": {
    "id": "6ef5e438"
   },
   "source": [
    "As we can see the mapping is not complete : Some Freebase IDs do not correspond to any Wikidata ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f7dda",
   "metadata": {
    "id": "f89f7dda",
    "outputId": "ffed60be-4ef0-4bd1-e0a7-6d91b729c017"
   },
   "outputs": [],
   "source": [
    "unmapped_ethnicities = characters.Ethnicity_W.isna().sum() - characters.FreeEthnicityID.isna().sum()\n",
    "print(\"There are {} instances of ethnicities that we could not map from Freebase to Wikidata\".format(unmapped_ethnicities))\n",
    "# Instances of ethnicities -> Can we replace by number of actors ethnicities? Actors with different initial ethnicities ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106ab7a",
   "metadata": {
    "id": "5106ab7a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a Wikidata ID (eg. \"Q49085\") returns the corresponding label \n",
    "\n",
    "Using this function row by row (pd.apply method) takes enormous time -> improvement? For ethnicities its ok as they're only 350 dinstinct ones\n",
    "\"\"\"\n",
    "def get_wikidata_label(wikidata_id):\n",
    "    client = Client()\n",
    "    entity = client.get(wikidata_id, load=True)\n",
    "    return str(entity.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2b302",
   "metadata": {
    "id": "b0d2b302"
   },
   "outputs": [],
   "source": [
    "# Result is map_Ethnicity_W_to_Ethnicity_name a map between Ethnicity wikidata ID and Ethnicity label\n",
    "map_Ethnicity_W_to_Ethnicity_name = pd.DataFrame(characters.Ethnicity_W.unique(), columns=[\"Ethnicity_W\"]).dropna()\n",
    "\n",
    "# Getting the equivalent label for each Wikidata ID\n",
    "map_Ethnicity_W_to_Ethnicity_name[\"EthnicityName\"] = map_Ethnicity_W_to_Ethnicity_name.Ethnicity_W.apply(get_wikidata_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ae846",
   "metadata": {
    "id": "5e0ae846",
    "outputId": "8ec0317d-1665-42d2-bca7-17a55bc06008"
   },
   "outputs": [],
   "source": [
    "# Assigining the missing ethnicities\n",
    "characters = characters.merge(map_Ethnicity_W_to_Ethnicity_name, how=\"left\", left_on=\"Ethnicity_W\", right_on=\"Ethnicity_W\").drop(columns= [\"FreeEthnicityID\", \"Ethnicity_W\"])\n",
    "display(characters.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff777ce",
   "metadata": {
    "id": "0ff777ce",
    "outputId": "a266c96e-a477-4563-9e1e-83cb915dea09"
   },
   "outputs": [],
   "source": [
    "ethn_len = characters.EthnicityName.nunique()\n",
    "print(\"Number of unique ethnicities : {}\".format(ethn_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbc368",
   "metadata": {
    "id": "4bfbc368"
   },
   "source": [
    "The whole ethnicities plot (355 different ethnicities) was not really readable, so we plotted what are the most present ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b036e",
   "metadata": {
    "id": "9a2b036e",
    "outputId": "c813afb2-a59b-4845-dd7c-31629e0817af"
   },
   "outputs": [],
   "source": [
    "# Ethnicities visualization\n",
    "plt.figure(figsize=(10,5))\n",
    "characters.EthnicityName.value_counts()[:20].plot(kind=\"bar\")\n",
    "plt.xlabel('Ethnicities')\n",
    "plt.ylabel('Number of actors')\n",
    "plt.title('Distribution of the twenty most represented ethnicities');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78494a52",
   "metadata": {
    "id": "78494a52",
    "outputId": "b06428d7-cb29-4c69-8cb9-abe4aeb4b5d5"
   },
   "outputs": [],
   "source": [
    "print('The percentage of missing Ethnicity names is :', round(100*characters[['EthnicityName']].isna().sum().values[0]/n_char,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe362b",
   "metadata": {
    "id": "04fe362b"
   },
   "source": [
    "We only have the names of 20% of the actor ethnicities : this is not enough to base our analysis and to draw conclusions. We thus decided to not use the Ethnicity in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab7127",
   "metadata": {
    "id": "72ab7127",
    "tags": []
   },
   "source": [
    "### 1.4.2 Recovering Missing Actor Names  <a id='1.4.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0496b2",
   "metadata": {
    "id": "1b0496b2"
   },
   "source": [
    "In addition, some actors do not have their name and/or date of birth and/or sex specified but the actor freebase ID is present. We can use the actor freebase ID to recover their Wikidata information. This gives us access to the following relevant information about the actor :\n",
    "- Name\n",
    "- Sex/Gender\n",
    "- Date of Birth  \n",
    "We can therefore replace the missing values in those categories, when the Actor Freebase ID is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb4d20",
   "metadata": {
    "id": "85cb4d20",
    "outputId": "a39de198-56fd-440c-f166-1ceed720435d"
   },
   "outputs": [],
   "source": [
    "print('Number of rows where:\\n\\tActor is missing but actor ID is specified :', len(characters[(characters['ActorName'].isna() & ~characters['FreeActorID'].isna())]))\n",
    "print('\\tActor and DOB are missing but actor ID is specified :', len(characters[(characters['ActorName'].isna() & characters['ActorDOB'].isna() & ~characters['FreeActorID'].isna())]))\n",
    "print('\\tActor and Gender are missing but actor ID is specified :', len(characters[(characters['ActorName'].isna() & characters['ActorGender'].isna() & ~characters['FreeActorID'].isna())]))\n",
    "print('\\tActor, DOB and Gender are missing but actor ID is specified :', len(characters[(characters['ActorName'].isna() & characters['ActorDOB'].isna() & characters['ActorGender'].isna() & ~characters['FreeActorID'].isna())]))\n",
    "print('\\tDOB is missing but Actor and actor ID are specified :', len(characters[(~characters['ActorName'].isna() & characters['ActorDOB'].isna() & ~characters['FreeActorID'].isna())]))\n",
    "print('\\tGender is missing but Actor and actor ID are specified :', len(characters[(~characters['ActorName'].isna() & characters['ActorGender'].isna() & ~characters['FreeActorID'].isna())]))\n",
    "print('\\tDOB and Gender are missing but Actor and actor ID are specified :', len(characters[(~characters['ActorName'].isna() & characters['ActorDOB'].isna() & characters['ActorGender'] & ~characters['FreeActorID'].isna())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09cec5b",
   "metadata": {
    "id": "f09cec5b"
   },
   "source": [
    "#### Starting with recovery of missing Actor names\n",
    "\n",
    "For this, we perform the same as in the Ethnicity names recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a2774",
   "metadata": {
    "id": "3a1a2774",
    "outputId": "d5dc4850-0871-4888-d182-61ce1f07e99a"
   },
   "outputs": [],
   "source": [
    "characters = characters.merge(maps, how=\"left\", left_on=\"FreeActorID\", right_on=\"free\")\n",
    "characters.drop(columns=[\"FreeActorID\"], inplace=True)\n",
    "characters.rename(columns={\"wiki\":\"WikiActorID\"}, inplace=True)\n",
    "display(characters.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6698ba2",
   "metadata": {
    "id": "f6698ba2"
   },
   "outputs": [],
   "source": [
    "missing_actor = characters[(characters['ActorName'].isna() & ~characters['WikiActorID'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ee4a7",
   "metadata": {
    "id": "b39ee4a7"
   },
   "outputs": [],
   "source": [
    "map_WikiActorId_Actor = pd.DataFrame(missing_actor.WikiActorID.unique(), columns=[\"WikiActorID\"]).dropna()\n",
    "map_WikiActorId_Actor[\"ActorName\"] = map_WikiActorId_Actor.WikiActorID.apply(get_wikidata_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896aa97",
   "metadata": {
    "id": "3896aa97"
   },
   "outputs": [],
   "source": [
    "characters = characters.merge(map_WikiActorId_Actor, how=\"left\", left_on=\"WikiActorID\", right_on=\"WikiActorID\").drop(columns= [\"WikiActorID\", \"WikiActorID\"])\n",
    "characters.drop(columns=\"free\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d77b9",
   "metadata": {
    "id": "5b9d77b9"
   },
   "outputs": [],
   "source": [
    "characters[\"ActorName\"] = characters.fillna(value={\"ActorName_x\":\"\"}).ActorName_x + characters.fillna(value={\"ActorName_y\":\"\"}).ActorName_y\n",
    "characters.drop(columns=[\"ActorName_x\", \"ActorName_y\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e947d5d",
   "metadata": {
    "id": "2e947d5d",
    "outputId": "61172fd1-b15b-4278-fd91-27391916dbf3"
   },
   "outputs": [],
   "source": [
    "characters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d4f25",
   "metadata": {
    "id": "299d4f25"
   },
   "source": [
    "From wikidata actor page,\n",
    "- sex/gender (P21) is given as : male (Q6581097), female (Q6581072), intersex (Q1097630), transgender female (Q1052281), transgender male (Q2449503).\n",
    "- date of birth (P569) is given as : Day of the month as a zero-padded decimal number, blank space, Month full name, blank space, Year with century as a decimal number. To recover the date of birth in the same format as the ones in the characters dataset, we must read the date given by wikidata without the blank spaces and then use datetime to format it correctly. See example just below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7760b8",
   "metadata": {
    "id": "6f7760b8",
    "outputId": "37ed52d8-2808-4b66-8839-cfa07777c93a"
   },
   "outputs": [],
   "source": [
    "wiki_date = '26 December 1949'\n",
    "no_space = wiki_date.replace(\" \", \"\")\n",
    "date = datetime.strptime(no_space, '%d''%B''%Y')\n",
    "formated_date = '{:%Y-%m-%d}'.format(date)\n",
    "print(formated_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b5ca65",
   "metadata": {
    "id": "64b5ca65"
   },
   "source": [
    "We later decided that we won't use the actors information, so we didn't push further and didn't recover missing DOB or genders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13684f",
   "metadata": {
    "id": "ef13684f"
   },
   "source": [
    "### Saving augmented characters dataset\n",
    "\n",
    "Extracting from wikidata takes a few minutes so we can save our final `characters` dataframe in case we want to use that directly in further applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf48c5",
   "metadata": {
    "id": "bccf48c5"
   },
   "outputs": [],
   "source": [
    "os.makedirs('data', exist_ok=True)  \n",
    "characters.to_csv('data/characters_aug.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3720d7",
   "metadata": {
    "id": "4d3720d7",
    "tags": []
   },
   "source": [
    "# 2. Plot Summaries   <a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98317c17-f90b-4e20-8fda-361c43c43ab3",
   "metadata": {
    "id": "98317c17-f90b-4e20-8fda-361c43c43ab3",
    "tags": []
   },
   "source": [
    "The plot summaries are loaded from the file `plot_summaries.txt` and stored into `plot_summaries`dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6962e3-2f02-46aa-a34a-d4ba8110a6e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 2103,
     "status": "ok",
     "timestamp": 1668727374787,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "2b6962e3-2f02-46aa-a34a-d4ba8110a6e2",
    "outputId": "b34e2b12-53b2-454a-b44e-03531e371dbe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_summaries = pd.read_csv('data/plot_summaries.txt', sep=\"\\t\", header=None,names=[\"WikiMovieID\", \"Plot\"])\n",
    "plot_summaries.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c393a17",
   "metadata": {
    "id": "0c393a17",
    "tags": []
   },
   "source": [
    "## 2.1 Preprocessing  <a id='2.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908b664",
   "metadata": {
    "id": "f908b664",
    "tags": []
   },
   "source": [
    "### 2.1.1 Missing plot summaries check  <a id='2.1.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4e357-2742-470b-8789-834776e5883d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1668727380164,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "cca4e357-2742-470b-8789-834776e5883d",
    "outputId": "65f772ae-e8b7-4d9d-a771-2e939a80b07e"
   },
   "outputs": [],
   "source": [
    "print('Number of missing plot summary in the plot_summary dataset: {}'.format(plot_summaries.Plot.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a771f9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1668727382480,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "7a771f9b",
    "outputId": "6a637e18-15fc-49c0-e3ec-c54a890555ce"
   },
   "outputs": [],
   "source": [
    "print('Number of missing plot summary when matched to films in movies dataset: {:.2f}%'.format((movies.merge(plot_summaries, how=\"left\", on='WikiMovieID')).Plot.isna().sum()/len(movies)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5f931",
   "metadata": {
    "id": "86f5f931"
   },
   "source": [
    "We can see that we don't have the plot summary for about half of the movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92434f63",
   "metadata": {
    "id": "92434f63",
    "tags": []
   },
   "source": [
    "### 2.1.2 Cleaning <a id='2.1.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e2c3a",
   "metadata": {
    "id": "823e2c3a"
   },
   "source": [
    "We detect the langage used in the plot summary and keep only the ones written in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c3720",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 437962,
     "status": "ok",
     "timestamp": 1668727850976,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "242c3720",
    "outputId": "d37a7f17-2a9a-4530-c1d6-c1a0f10d20b2"
   },
   "outputs": [],
   "source": [
    "plot_summaries['lang'] = plot_summaries.Plot.progress_apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c366f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1668727850982,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "0f8c366f",
    "outputId": "71020c0f-c35f-46a9-e934-ed4230f6df6d"
   },
   "outputs": [],
   "source": [
    "plot_summaries.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d68739",
   "metadata": {
    "id": "71d68739",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_summaries = plot_summaries.loc[plot_summaries.lang=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de679864",
   "metadata": {
    "id": "de679864",
    "tags": []
   },
   "source": [
    "### 2.1.3 Tokenization <a id='2.1.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b1aa5",
   "metadata": {
    "id": "896b1aa5"
   },
   "source": [
    "Tokenizers are used to divide strings into lists of substrings. For each `Plot` in `plot_summaries` dataset, a list of words and punctuation marks stored in the `words_punc` column, a list of sentences in `sentences` and a list of tokenized sentences `tokens_sentences`. To do so, we use the natural langage processing library `NLTK` (Natural Language Toolkit). \"NLTK is a leading platform for building Python programs to work with human language data\"[[3]](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cee4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 267920,
     "status": "ok",
     "timestamp": 1668728126825,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "9c9cee4d",
    "outputId": "0a6af131-95b2-4672-d7d4-f4ac2011272c"
   },
   "outputs": [],
   "source": [
    "plot_summaries['words_punc'] = plot_summaries.Plot.progress_apply(lambda x: word_tokenize(x))\n",
    "plot_summaries['sentences'] = plot_summaries.Plot.progress_apply(lambda x: sent_tokenize(x))\n",
    "plot_summaries['tokens_sentences'] = plot_summaries['sentences'].progress_apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c2c8e",
   "metadata": {
    "id": "d60c2c8e",
    "tags": []
   },
   "source": [
    "### 2.1.4 PoS tagging <a id='2.1.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a93d6e",
   "metadata": {
    "id": "51a93d6e"
   },
   "source": [
    "Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context. POS tags from the [*Penn Treebank* tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) are used to describe the lexical terms that we have within our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded4963",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 842792,
     "status": "ok",
     "timestamp": 1668728971651,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "0ded4963",
    "outputId": "94504498-251c-49bf-eddf-30df85b3f62e"
   },
   "outputs": [],
   "source": [
    "plot_summaries['POS_tokens'] = plot_summaries['tokens_sentences'].progress_apply(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "\n",
    "plot_summaries['POS_tokens'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678b890",
   "metadata": {
    "id": "c678b890",
    "tags": []
   },
   "source": [
    "### 2.1.5 Lemmatization <a id='2.1.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4816542",
   "metadata": {
    "id": "f4816542"
   },
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. It links words with similar meanings to one word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821b30d",
   "metadata": {
    "id": "1821b30d"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479400a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "executionInfo": {
     "elapsed": 61727,
     "status": "ok",
     "timestamp": 1668729033360,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "2479400a",
    "outputId": "21255042-e125-4de7-9650-0b42eb5bb7f0"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizing each word with its POS tag, in each sentence\n",
    "plot_summaries['tokens_sentences_lemmatized'] = plot_summaries['POS_tokens'].progress_apply(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")\n",
    "\n",
    "plot_summaries[['Plot','tokens_sentences_lemmatized']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473b965",
   "metadata": {
    "id": "e473b965",
    "tags": []
   },
   "source": [
    "### 2.1.6 Grouping tokens and removing stop words <a id='2.1.6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca0181",
   "metadata": {
    "id": "82ca0181"
   },
   "source": [
    "In order to extact interesting information from the words present in the plot summaries, we have to take into account only the meaningful words by removing insignificant ones called stop words.\n",
    "What we have seen is that the names of the characters in the plot summaries create a lot of unwanted noise which in turn disturbs greatly our results in the topic extraction methods.Therefore, we introduce the [name-dataset](https://pypi.org/project/names-dataset/), which used a Facebook dump to create a first and last name database. From there  we extract the top 2000 first names and use them as stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260785d3",
   "metadata": {
    "id": "260785d3"
   },
   "outputs": [],
   "source": [
    "nd = NameDataset()\n",
    "names = nd.get_top_names(n=2000, country_alpha2=\"US\")\n",
    "names = names[\"US\"][\"M\"] + names[\"US\"][\"F\"]\n",
    "names = [name.lower() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6bdae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1668729054317,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "b4b6bdae",
    "outputId": "b44cb836-5e9b-4026-d6c8-94e4da8395c1"
   },
   "outputs": [],
   "source": [
    "# Words gathered after running the LDA model and collecting the words appearing in more than 4 topics \n",
    "stops = [\"one\", \"two\", \"also\", \"see\", \"take\", \"get\", \"find\", \"try\", \"however\", \"go\", \"come\", \"leave\", \"become\", \"make\", \"back\", \"run\"]\n",
    "my_stopwords = stopwords.words('english') + names + stops\n",
    "\n",
    "plot_summaries['tokens'] = plot_summaries['tokens_sentences_lemmatized'].progress_apply(lambda sentences: list(chain.from_iterable(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a6f88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544810,
     "status": "ok",
     "timestamp": 1668729599113,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "2a4a6f88",
    "outputId": "05f9ed57-70dd-4c43-862e-0856819f8465"
   },
   "outputs": [],
   "source": [
    "plot_summaries['tokens'] = plot_summaries['tokens'].progress_apply(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74aecfd-349e-459c-b5f3-d4c60fac8b19",
   "metadata": {},
   "source": [
    "Another idea, would be to use PoS tagging to remove the all names from the plots instead of using the name-dataset. For example names that are not recognized using the name-dataset would be recognized using proper noun tags (NNP and NNPS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vwijcd6v9W1t",
   "metadata": {
    "id": "vwijcd6v9W1t"
   },
   "source": [
    "Saving plot summaries dataframe to access it later without performing again the previous steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mt1vmEwl9feb",
   "metadata": {
    "id": "Mt1vmEwl9feb"
   },
   "outputs": [],
   "source": [
    "plot_summaries.to_pickle('data/plot_summaries_aug.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3dc48",
   "metadata": {
    "id": "5cf3dc48",
    "tags": []
   },
   "source": [
    "### 2.1.7 Integration into movies dataset <a id='2.1.7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rL0hjuLj-Dq7",
   "metadata": {
    "executionInfo": {
     "elapsed": 24549,
     "status": "ok",
     "timestamp": 1668793859563,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "rL0hjuLj-Dq7"
   },
   "outputs": [],
   "source": [
    "plot_summaries = pd.read_pickle('data/plot_summaries_aug.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77f136-694c-4e85-8614-8452952cbbff",
   "metadata": {
    "id": "6e77f136-694c-4e85-8614-8452952cbbff"
   },
   "source": [
    "The preprocessed plot summaries `plot_summaries` are added to the `movies` dataset according to the Wikipedia movie ID, `WikiMovieID`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985339f",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1668793859572,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "8985339f"
   },
   "outputs": [],
   "source": [
    "movies = movies.merge(plot_summaries, how=\"left\", on='WikiMovieID') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef38951-04db-4641-bd60-245ca994a54f",
   "metadata": {
    "id": "2ef38951-04db-4641-bd60-245ca994a54f"
   },
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634bf95-ab59-44f9-8e3a-f5fcc9e4b256",
   "metadata": {
    "id": "1634bf95-ab59-44f9-8e3a-f5fcc9e4b256"
   },
   "source": [
    "In part 2.1.1, we see that there is nearly half of the movies that has missing plot summaries. At this stage, we are interested in the plot summaries content so we remove the rows with missing plot summaries in `movies` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y8FBJ0YZC73K",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1668793863461,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "Y8FBJ0YZC73K"
   },
   "outputs": [],
   "source": [
    "movies = movies.dropna(subset=['Plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865089d-c56f-4fe4-8312-f6a4405889ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1668793865494,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "9865089d-c56f-4fe4-8312-f6a4405889ab",
    "outputId": "8851437c-9637-48b6-cc7a-4f0d616d091a"
   },
   "outputs": [],
   "source": [
    "print('Length of movies dataset: {}'.format(len(movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ea17b-e28a-40b8-9782-f0d078ca9015",
   "metadata": {
    "id": "658ea17b-e28a-40b8-9782-f0d078ca9015"
   },
   "source": [
    "### 2.1.8 Preprocessing results <a id='2.1.8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a99c29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1668793867826,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "c8a99c29",
    "outputId": "e6d4d389-4152-4c43-f789-9fb7def8b94c"
   },
   "outputs": [],
   "source": [
    "movies[['Plot','words_punc','sentences','tokens_sentences','tokens']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a23b34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1668793871088,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "46a23b34",
    "outputId": "caadba45-f6d2-4a56-904b-2913a681a7a3"
   },
   "outputs": [],
   "source": [
    "print(movies['Plot'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bd599",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1668793872151,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "860bd599",
    "outputId": "f57f5e51-b8cf-4e0d-faf6-35bc76619118"
   },
   "outputs": [],
   "source": [
    "print(movies['POS_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28708c37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1668793875527,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "28708c37",
    "outputId": "cb581783-920d-4c40-fa62-aa886b49db61"
   },
   "outputs": [],
   "source": [
    "print(movies['tokens_sentences_lemmatized'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad52ee3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1668793875964,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "cad52ee3",
    "outputId": "27d8a3da-c02a-493c-a9d3-2a7717d5e9b3"
   },
   "outputs": [],
   "source": [
    "print(movies['tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ecdb6",
   "metadata": {
    "id": "cf3ecdb6",
    "tags": []
   },
   "source": [
    "## 2.2 Importing Metascore  <a id='2.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284815f",
   "metadata": {
    "id": "a284815f"
   },
   "source": [
    "When considering film rating, we can think about using the IMDb database. In particular, it provides the variable `averageRating` which is the weighted average of all the individual user ratings. However, IMDb user opinions can be given whenever the user wants. So, the ratings of IMDb do not match with the release date of the movie. It only works for analyzing the current opinions, as very recent reviews and can be written for an old film. \n",
    "\n",
    "In order to have an insigth into the movie impression at a time closer to the release date, we can consider metascore. [Metascore](https://github.com/miazhx/metacritic) is a weighted average of reviews from top published critic reviews for a given movie, and thus does not include any votes or comments from our users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bfb38-e7d7-44d6-906d-c0afe62a2574",
   "metadata": {
    "id": "694bfb38-e7d7-44d6-906d-c0afe62a2574"
   },
   "source": [
    "#### Metacritic dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f0855",
   "metadata": {
    "id": "400f0855"
   },
   "source": [
    "We use a dataset which brings together the film title, release date, metascore and other scores. We load it and add the `metascore` information into the `movie` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd623c12",
   "metadata": {
    "executionInfo": {
     "elapsed": 1029,
     "status": "ok",
     "timestamp": 1668793882623,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "bd623c12"
   },
   "outputs": [],
   "source": [
    "metacritic_url = \"https://raw.githubusercontent.com/miazhx/metacritic/master/data/metacritic_movies.csv\"\n",
    "\n",
    "metacritic = pd.read_csv(metacritic_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68bc44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1668793887051,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "af68bc44",
    "outputId": "e4aceea7-d9e8-46a7-b648-c05daf81f149"
   },
   "outputs": [],
   "source": [
    "metacritic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b891b-2439-4687-a6d4-143c61a3abd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1668793891330,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "e67b891b-2439-4687-a6d4-143c61a3abd9",
    "outputId": "6dc41484-3c9c-4294-9f24-6f4f1d1a53c0"
   },
   "outputs": [],
   "source": [
    "print('Number of rows in metacrtic dataset: {}'.format(len(metacritic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785b867-462f-434b-8f9c-0550a7295133",
   "metadata": {
    "id": "f785b867-462f-434b-8f9c-0550a7295133"
   },
   "source": [
    "#### Missing value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106617f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1668793892679,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "4106617f",
    "outputId": "c6257298-7917-4ad1-9344-3f2779d9f1aa"
   },
   "outputs": [],
   "source": [
    "print('Number of missing metascore in metacritic dataset: {}'.format(metacritic['metascore'].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa609522-7b08-489e-9b82-fe4aec74cb91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1668793895656,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "fa609522-7b08-489e-9b82-fe4aec74cb91",
    "outputId": "2f382aaa-a921-4c45-a1f7-d75c26e8932e"
   },
   "outputs": [],
   "source": [
    "print('Number of missing title in metacritic dataset: {}'.format(metacritic['movie_title'].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014281ff-59db-414f-8ea6-a0a0e99a00b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1668793897026,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "014281ff-59db-414f-8ea6-a0a0e99a00b8",
    "outputId": "12a09741-85b3-4232-c1e9-855c3d383b71"
   },
   "outputs": [],
   "source": [
    "print('There are {} metascores for {} movies with plot summary available.'.format(len(metacritic),len(movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f271b4f-04a6-4bcc-a308-21cbd6efd3b0",
   "metadata": {
    "id": "3f271b4f-04a6-4bcc-a308-21cbd6efd3b0"
   },
   "source": [
    "#### Integration of metascores to `movies` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f5916-ba6a-44ad-b62f-f599a1452fae",
   "metadata": {
    "id": "246f5916-ba6a-44ad-b62f-f599a1452fae"
   },
   "source": [
    "In `metacritic` data set we are only interested in ratings for now. For each movie, we want to add the corresponding metascore if available. Unfortunately the `metacritic` data set does not have WikiID or unique identifier for the movie. Also, titles are not unique so we cannot match the two datasets according to this feature. One option is to match the data according to the title and the release date. In `metacritic` the release year is given by only 2 digits. So the data are matched in an non optimal way by comparing the title and the two last digits of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988b0dc-2ddf-4734-9575-97496e1367aa",
   "metadata": {
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1668793901714,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "e988b0dc-2ddf-4734-9575-97496e1367aa"
   },
   "outputs": [],
   "source": [
    "#Get informative subdataset, convert release date to datetime\n",
    "metacritic_score = metacritic[['movie_title','metascore','release_date']].rename(columns = {'movie_title':'Title'})\n",
    "metacritic_score['release_date_'] = pd.to_datetime(metacritic_score['release_date'], format='%d-%b-%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d15d40-6c2a-48ee-8f60-e96b8933680c",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1668793903300,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "42d15d40-6c2a-48ee-8f60-e96b8933680c"
   },
   "outputs": [],
   "source": [
    "#Get the two last digits, trying to deal with the misconvertion of years due to the initial format with only two digits for the year\n",
    "metacritic_score['year_last_digits'] = metacritic_score['release_date_'].apply( lambda x: int(x.year-100) if (x.year >2022) else int(x.year))\n",
    "metacritic_score['year_last_digits'] = metacritic_score['year_last_digits'].apply( lambda x: x-2000 if (x > 1999) else x-1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dd741-79e4-473b-b2df-2a7951c24bc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1668793906648,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "286dd741-79e4-473b-b2df-2a7951c24bc4",
    "outputId": "eacffcae-af99-4ada-bafa-c5f91b87fbe2"
   },
   "outputs": [],
   "source": [
    "#Get the two last digits of the year in movies dataset\n",
    "movies['ReleaseDate_'] = pd.to_datetime(movies['ReleaseDate'], format=\"%Y-%m-%d\", errors=\"coerce\").fillna(pd.to_datetime(movies['ReleaseDate'], format=\"%Y\", errors=\"coerce\"))\n",
    "movies['year_last_digits'] = movies['ReleaseDate_'].apply( lambda x: int(str(x.year)[2:4]) if ( pd.notnull(x) ) else x)\n",
    "movies[['ReleaseDate_','year_last_digits']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6399a7-e41d-4dd2-8040-93ae8a74cbd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1668793917743,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "bb6399a7-e41d-4dd2-8040-93ae8a74cbd4",
    "outputId": "9b90286b-782f-4d95-a00a-63cc37ffd11a"
   },
   "outputs": [],
   "source": [
    "metacritic_score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb5a32-fcad-4c6b-ac2a-dbe4a32bbf2e",
   "metadata": {
    "id": "cadb5a32-fcad-4c6b-ac2a-dbe4a32bbf2e"
   },
   "source": [
    "Adding metascores to the `movies` dataset by merging on `Tilte`and release year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fafd68-d275-4179-a099-19c405911cd5",
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1668793924499,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "80fafd68-d275-4179-a099-19c405911cd5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies = movies.merge(metacritic_score[['Title','metascore','year_last_digits']], how=\"left\", on=['Title','year_last_digits'] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb1453-2955-417d-a502-a076443eacd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1668793927997,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "3dfb1453-2955-417d-a502-a076443eacd4",
    "outputId": "197289f8-7755-4e47-f966-13bb69ee62d4"
   },
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176b17a-94be-4aae-8612-0534f493b579",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1668793934127,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "f176b17a-94be-4aae-8612-0534f493b579",
    "outputId": "27eb5324-53a7-40a0-c431-b5791027c2d2"
   },
   "outputs": [],
   "source": [
    "print('We were able to retreive {} metascores.'.format(len(movies)-movies.metascore.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f57516-d4bf-4444-9c74-25d2153f7db5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1668793936593,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "08f57516-d4bf-4444-9c74-25d2153f7db5",
    "outputId": "025b03b5-b211-4a07-9194-b95e1774b76d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Missing metascores in movies dataset: {:.2f}%\".format(movies['metascore'].isna().sum()/len(movies)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c584da-ac2d-4eda-a50f-b1142e89d64c",
   "metadata": {
    "id": "e2c584da-ac2d-4eda-a50f-b1142e89d64c"
   },
   "source": [
    "With 9121 metascores initially, we could have hoped to have about 20% of our movies with plot augmented with metascores. In the end, we only managed to match half of these metascores which represents about 10% of the movies with plot summaries available.  \n",
    "This result is not entirely satisfying because a lot of metascores are missing. If we want to increase this percentage, we should fetch data from the web (IMDb website has metascores), we will see this in Milestone 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f440d",
   "metadata": {
    "id": "4f0f440d",
    "tags": []
   },
   "source": [
    "## 2.3 Initial analysis  <a id='2.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78ad43",
   "metadata": {
    "id": "0f78ad43",
    "tags": []
   },
   "source": [
    "### 2.3.1 Plot structure <a id='2.3.1'></a>\n",
    "\n",
    "Here, for each plot summary, we investigate the number of sentences, number of words, numbers of the different punctuation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84136c-0550-4734-9a34-f029cf94ae4c",
   "metadata": {
    "id": "bf84136c-0550-4734-9a34-f029cf94ae4c"
   },
   "source": [
    "We add new columns to the `movies` dataframe by counting, in each plot summary, the number of sentences, words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y8H86yPaDV7L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 6001,
     "status": "ok",
     "timestamp": 1668793954632,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "Y8H86yPaDV7L",
    "outputId": "abaa7b2b-bcfa-44b5-b3a2-0b5d7f6f0f6a"
   },
   "outputs": [],
   "source": [
    "movies['plot_num_sentences'] = movies.sentences.apply( lambda x: len(x))\n",
    "movies['plot_num_words'] =  movies.Plot.apply( lambda x: len(RegexpTokenizer(r'\\w+').tokenize(x)))\n",
    "movies['plot_num_dot'] = movies.words_punc.apply( lambda x: list(x).count('.'))\n",
    "movies['plot_num_coma'] = movies.words_punc.apply( lambda x: x.count(','))\n",
    "movies['plot_num_interrogation'] = movies.words_punc.apply( lambda x: x.count('?'))\n",
    "movies['plot_num_exclamation'] = movies.words_punc.apply( lambda x: x.count('!'))\n",
    "movies['plot_num_percentage'] = movies.words_punc.apply( lambda x: x.count('%'))\n",
    "movies['plot_num_semicolon'] = movies.words_punc.apply( lambda x: x.count(';'))\n",
    "movies['plot_num_colon'] = movies.words_punc.apply( lambda x: x.count(':'))\n",
    "\n",
    "movies[['plot_num_sentences','plot_num_words','plot_num_dot','plot_num_coma','plot_num_interrogation',\\\n",
    "        'plot_num_exclamation','plot_num_percentage','plot_num_semicolon','plot_num_colon']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ac363",
   "metadata": {
    "id": "8a1ac363",
    "tags": []
   },
   "source": [
    "#### 2.3.1.1 Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638b53e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1668793954635,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "5638b53e",
    "outputId": "aa86d9a7-ce25-4b5d-ab9b-5f35316b4b7d"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "movies[['plot_num_sentences','plot_num_words','plot_num_dot','plot_num_coma','plot_num_interrogation',\n",
    "        'plot_num_exclamation','plot_num_percentage','plot_num_semicolon','plot_num_colon']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345460aa-4d58-4fa5-84cb-32672c4e89d1",
   "metadata": {
    "id": "345460aa-4d58-4fa5-84cb-32672c4e89d1",
    "tags": []
   },
   "source": [
    "#### 2.3.1.2 Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed875a8-5d8e-44b5-8e98-855bb334d3ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1668793955611,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "4ed875a8-5d8e-44b5-8e98-855bb334d3ed",
    "outputId": "d8103cdc-c96d-4efb-c6c1-5755b0450664"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14,6))\n",
    "fig.tight_layout(pad=3)\n",
    "axs[0].hist(movies.plot_num_sentences,bins=40,log=True)\n",
    "axs[0].set_title('Distribution of sentences number \\nacross plot summaries')\n",
    "axs[0].set_xlabel('Number of sentences')\n",
    "axs[0].set_ylabel('Counts (log)')\n",
    "\n",
    "axs[1].hist(movies.plot_num_words,bins=40,log=True)\n",
    "axs[1].set_title('Distribution of words number \\nacross plot summaries')\n",
    "axs[1].set_xlabel('Number of words')\n",
    "axs[1].set_ylabel('Counts (log)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efef71-dfc7-4eee-a772-e202cea123b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 1722,
     "status": "ok",
     "timestamp": 1668794052658,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "07efef71-dfc7-4eee-a772-e202cea123b0",
    "outputId": "24dcc64b-3908-449b-fba6-ed151044d413"
   },
   "outputs": [],
   "source": [
    "punctuation1 = movies[['plot_num_coma','plot_num_dot']]\n",
    "punctuation1.plot.hist(bins=40, log=True, alpha=0.5, figsize = (8,6))\n",
    "plt.title('Distribution of coma and dot number across plot summaries')\n",
    "plt.xlabel('Number of punctuation mark')\n",
    "plt.ylabel('Counts (log)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a899e-2888-489b-93e8-6c0936a876da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 2498,
     "status": "ok",
     "timestamp": 1668794068138,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "8a3a899e-2888-489b-93e8-6c0936a876da",
    "outputId": "58bb399c-bc68-46a2-8393-6fe8f22dd85e"
   },
   "outputs": [],
   "source": [
    "punctuation2 = movies[['plot_num_semicolon','plot_num_colon','plot_num_exclamation','plot_num_percentage','plot_num_interrogation']]\n",
    "punctuation2.plot.hist(bins=40, log=True, alpha=0.5, figsize = (8,6))\n",
    "plt.title('Distribution of exclamation points, interrogation points, semicolon, colon, \\nand percent symbol number across plot summaries')\n",
    "plt.xlabel('Number of punctuation mark')\n",
    "plt.ylabel('Counts (log)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QYdeeKph4RAr",
   "metadata": {
    "id": "QYdeeKph4RAr"
   },
   "source": [
    "We observe in general that punctuation distributions follow a heavy-tailed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4458f",
   "metadata": {
    "id": "b3c4458f",
    "tags": []
   },
   "source": [
    "### 2.3.2 Most common tokens in plot summaries <a id='2.3.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7116b",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1668794144264,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "97f7116b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the n most common elment of a list\n",
    "Inputs: \n",
    "     n[int]: number of most common elements\n",
    "     List of interest\n",
    "Ouptuts:\n",
    "    List of n tuples liking the element and its number of occurence(s)\n",
    "\"\"\"\n",
    "def most_frequent(List,n):\n",
    "    occurence_count = Counter(List)\n",
    "    return occurence_count.most_common(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37edb12-bdfa-4ea1-8b45-fb333798548d",
   "metadata": {
    "id": "c37edb12-bdfa-4ea1-8b45-fb333798548d"
   },
   "source": [
    "In each plot, the ten most used tokens are represented in the dataframe as a pair of token and its occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab78fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1982,
     "status": "ok",
     "timestamp": 1668794146708,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "19ab78fd",
    "outputId": "19d52a62-7624-43a0-bc6b-3bf953dbd899"
   },
   "outputs": [],
   "source": [
    "movies['plot_top_ten_tokens'] = movies.tokens.apply( lambda x: most_frequent(x,10))\n",
    "\n",
    "movies['plot_top_ten_tokens'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xEE41ga4iQuD",
   "metadata": {
    "id": "xEE41ga4iQuD"
   },
   "source": [
    "For the analysis of the most used token accross genre, we will remove the occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mOesb8T_M5y5",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1668794147081,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "mOesb8T_M5y5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transform a list provided by the function \"most_frequent\"\n",
    "Inputs: \n",
    "     List of tuples (element, occurence)\n",
    "Ouptuts:\n",
    "    List of element without occurence\n",
    "\"\"\"\n",
    "def remove_nb_usage(list_tokens):\n",
    "  # to remove the number of occurence of the 10 most used tokens\n",
    "    clean_tokens = []\n",
    "    for pair in list_tokens:\n",
    "        clean_tokens.append(pair[0])\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ra02UotANjYO",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1668794153301,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "ra02UotANjYO"
   },
   "outputs": [],
   "source": [
    "movies['clean_top_ten_tokens'] = movies['plot_top_ten_tokens'].apply(remove_nb_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r0glXTFiN0AW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1668794154858,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "r0glXTFiN0AW",
    "outputId": "dac62409-fd97-4c77-8dfe-15aabc41e07e"
   },
   "outputs": [],
   "source": [
    "movies['clean_top_ten_tokens'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3caa4-3993-4a54-955c-e77ddb2c9f9a",
   "metadata": {
    "id": "f4c3caa4-3993-4a54-955c-e77ddb2c9f9a"
   },
   "source": [
    "### 2.3.3 Movie genres clustering <a id='2.3.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f3629-eae7-4ae4-babc-82dde3a23038",
   "metadata": {
    "id": "ec7f3629-eae7-4ae4-babc-82dde3a23038"
   },
   "source": [
    "In the future, we would like to match movie genres with most used tokens, to see if some tokens are repeated. This analysis won't be performed in Milestone3, but could be a good lead for Milestone3. For now, we begin with a raw clustering of movie genres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0374982-ca38-47b4-9a2b-4921b88244e8",
   "metadata": {
    "id": "e0374982-ca38-47b4-9a2b-4921b88244e8"
   },
   "source": [
    "In the movies dataframe, `Genres` is usually a list with multiple genres or subgenres. First, we extract each individual genres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc059578-e7d6-400d-afea-7ffb6881f515",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1668794156962,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "bc059578-e7d6-400d-afea-7ffb6881f515"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get each element from lists inside of a pandas Series\n",
    "\"\"\"\n",
    "def to_1D(series):\n",
    "    return pd.Series([x for list_ in series for x in list_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47174e6e-47dd-4d12-992b-898aff649a5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1668794158073,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "47174e6e-47dd-4d12-992b-898aff649a5e",
    "outputId": "b5cf1e31-971b-478c-d0b4-b63036728397"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "individual_genres = to_1D(movies['Genres'])\n",
    "print(individual_genres.value_counts().index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b247d-e390-4d26-8213-3aa1ac209e61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1668794163991,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "a94b247d-e390-4d26-8213-3aa1ac209e61",
    "outputId": "20efa5db-5c44-4bc0-cd67-6ea2b38b5169"
   },
   "outputs": [],
   "source": [
    "print('There are', len(individual_genres.value_counts()), 'unique genres.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91c004-3c1e-4b8d-94ee-367a468f3916",
   "metadata": {
    "id": "af91c004-3c1e-4b8d-94ee-367a468f3916"
   },
   "source": [
    "For visualization purpose, we won't use the 363 different genres for now.  \n",
    "\n",
    "For now, we will create bigger theme/genre clusters.   Disclaimer : This is a first, raw extraction of bigger movie theme/genre, to have some insights about movie genres. However his grouping is not exhaustive and as not complete, may lead to bias in interpretation.\n",
    "\n",
    "To do so, we create 24 big clusters. A first cluster list will contain keywords that are present in the genre name. Then, we search if a genre is present in a cluster and add it to a final cluster list, that will contain the actual genre names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85ab68-733c-47c1-83bd-48f3480195b7",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1668794414550,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "3f85ab68-733c-47c1-83bd-48f3480195b7"
   },
   "outputs": [],
   "source": [
    "# lists containing cluster keywords \n",
    "comedy = ['comedy', 'comedies', 'humour', 'satire', 'parody', 'bloopers',\\\n",
    "          'gross', 'slapstick']\n",
    "drama = ['drama', 'tragedy']\n",
    "horror = ['horror', 'haunted', 'demonic', 'escape', 'splatter']\n",
    "romance = ['romance', 'romantic']\n",
    "criminal = ['thriller', 'crime', 'detective', 'gangset', 'criminal', 'giallo',\\\n",
    "            'heist', 'neo-noir', 'film noir'] \n",
    "action_adventure = ['action', 'adventure', 'war', 'spy', 'ninja', 'martial',\\\n",
    "                    'tokusatsu', 'outlaw', 'combat', 'buddy', 'biker', 'epic',\\\n",
    "                     'swashbuckler'] \n",
    "entertainment_hobby = ['family', 'children', 'sport', 'baseball', 'dance',\\\n",
    "                       'holiday', 'christmas', 'party', 'hip hop', 'beach']\n",
    "fiction = ['fiction', 'fictional', 'dystopia', 'future noir']\n",
    "erotism = ['erotic','pornography', 'erotica', 'adult', 'porn', 'homoeroticism',\\\n",
    "           'pinku']\n",
    "sci_fi = ['science fiction', 'sci-fi', 'alien', 'supernatural', 'time travel',\\\n",
    "          'space opera']\n",
    "animation = ['animation', 'anime', 'animated']\n",
    "musical = ['musical', 'music', 'concert', 'operetta', 'film-opera']\n",
    "fantasy = ['fantasy', 'fantasies', 'superhero', 'werewolf', 'sorcery', 'fairy',\\\n",
    "           'vampire', 'creature', 'zombie', 'monster', 'sword and sandal']\n",
    "western = ['western', 'cowboy']\n",
    "biography = ['biography', 'biographical']\n",
    "queer = ['gay', 'lgbt', 'queer']\n",
    "apocalypse = ['disaster', 'apocalyptic']\n",
    "historical = ['history', 'historical', 'cold war', 'gulf war', 'world war',\\\n",
    "              'empire', 'gladiators', 'inventions']\n",
    "society = ['social', 'prison', 'gender', 'media', 'society', 'disabilities',\\\n",
    "           'relationships', 'journalism', 'education', ]\n",
    "religion = ['christian', 'religious', 'hagiography']\n",
    "engaged = ['anti-war', 'feminist', 'political']\n",
    "ethnic = ['chinese', 'filipino', 'samurai', 'japanese', 'bollywood',\\\n",
    "          'latino', 'malayalam', 'world cinema', 'bengali', 'tollywood'\\\n",
    "          'czechoslovak', 'northern', 'early black', 'blaxploitation']\n",
    "low_budget = ['b-', 'c-', 'z movie', 'indie', 'sexploitation']\n",
    "_ism = ['surrealism', 'expressionism', 'neorealism', 'existentialism',\\\n",
    "        'realism', 'avant-garde', 'absurdism', 'kafkaesque', 'experimental']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14616d-2d33-4f10-995e-a5f157e4b24e",
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1668794419114,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "ed14616d-2d33-4f10-995e-a5f157e4b24e"
   },
   "outputs": [],
   "source": [
    "# lists that will contain actual genre names\n",
    "group_comedy, numbers_comedy = [], []\n",
    "group_drama, numbers_drama = [], []\n",
    "group_horror, numbers_horror = [], []\n",
    "group_romance, numbers_romance = [], []\n",
    "group_criminal, numbers_criminal = [], []\n",
    "group_action_adv, numbers_action_adv = [], []\n",
    "group_entert_hobb, numbers_entert_hobb = [], []\n",
    "group_fiction, numbers_fiction = [], []\n",
    "group_erotism, numbers_erotism = [], []\n",
    "group_scifi, numbers_scifi = [], []\n",
    "group_animation, numbers_animation = [], []\n",
    "group_musical, numbers_musical = [], []\n",
    "group_fantasy, numbers_fantasy = [], []\n",
    "group_western, numbers_western = [], []\n",
    "group_biography, numbers_biography = [], []\n",
    "group_queer, numbers_queer = [], []\n",
    "group_apocalypse, numbers_apocalypse = [], []\n",
    "group_history, numbers_hystory = [], []\n",
    "group_society, numbers_society = [], []\n",
    "group_religion, numbers_religion = [], []\n",
    "group_engaged, numbers_engaged = [], []\n",
    "group_world, numbers_world = [], []\n",
    "group_lowbud, numbers_lowbud = [], []\n",
    "group_ism, numbers_ism = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38761c2a-003e-4d27-82c8-a321e4d0fbfd",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1668794421463,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "38761c2a-003e-4d27-82c8-a321e4d0fbfd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Appends to a list movie genres that belongs to a same cluster.\n",
    "\"\"\"\n",
    "def find_cluster(cluster, genre, count, group, numbers):\n",
    "    for keyword in cluster :\n",
    "        if keyword in genre:\n",
    "            group.append(genre)\n",
    "            numbers.append(count)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815db22f-d531-4562-92fc-9406ced94d86",
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1668794465806,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "815db22f-d531-4562-92fc-9406ced94d86"
   },
   "outputs": [],
   "source": [
    "indiv_genres = [x.lower() for x in individual_genres.value_counts().index.to_numpy()]\n",
    "indiv_counts = [x for x in individual_genres.value_counts().values]\n",
    "for g, count in zip(indiv_genres, indiv_counts):\n",
    "    find_cluster(comedy, g, count, group_comedy, numbers_comedy)\n",
    "    find_cluster(drama, g, count, group_drama, numbers_drama)\n",
    "    find_cluster(horror, g, count, group_horror, numbers_horror)\n",
    "    find_cluster(romance, g, count, group_romance, numbers_romance)\n",
    "    find_cluster(criminal, g, count, group_criminal, numbers_criminal)\n",
    "    find_cluster(action_adventure, g, count, group_action_adv, numbers_action_adv)\n",
    "    find_cluster(entertainment_hobby, g, count, group_entert_hobb, numbers_entert_hobb)\n",
    "    find_cluster(fiction, g, count, group_fiction, numbers_fiction)\n",
    "    find_cluster(erotism, g, count, group_erotism, numbers_erotism)\n",
    "    find_cluster(sci_fi, g, count, group_scifi, numbers_scifi)\n",
    "    find_cluster(animation, g, count, group_animation, numbers_animation)\n",
    "    find_cluster(musical, g, count, group_musical, numbers_musical)\n",
    "    find_cluster(fantasy, g, count, group_fantasy, numbers_fantasy)\n",
    "    find_cluster(western, g, count, group_western, numbers_western)\n",
    "    find_cluster(biography, g, count, group_biography, numbers_biography)\n",
    "    find_cluster(queer, g, count, group_queer, numbers_queer)\n",
    "    find_cluster(apocalypse, g, count, group_apocalypse, numbers_apocalypse)\n",
    "    find_cluster(historical, g, count, group_history, numbers_hystory)\n",
    "    find_cluster(society, g, count, group_society, numbers_society)\n",
    "    find_cluster(religion, g, count, group_religion, numbers_religion)\n",
    "    find_cluster(engaged, g, count, group_engaged, numbers_engaged)\n",
    "    find_cluster(ethnic, g, count, group_world, numbers_world)\n",
    "    find_cluster(low_budget, g, count, group_lowbud, numbers_lowbud)\n",
    "    find_cluster(_ism, g, count, group_ism, numbers_ism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5ff26-b470-4308-9da3-c3b53192c149",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1668794471500,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "0ed5ff26-b470-4308-9da3-c3b53192c149"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create bar plots for each cluster of movie genres.\n",
    "\"\"\"\n",
    "def plot_clusters(ix, iy, group, counts, title, c):\n",
    "    axs[ix, iy].bar(group, counts, log=True, color=c)\n",
    "    axs[ix, iy].tick_params(labelrotation=90)\n",
    "    axs[ix, iy].set_ylabel('counts (log)')\n",
    "    axs[ix, iy].set_title(title+' movie genre cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125f85a-b3e6-4468-91b2-f0105f9d3a8f",
   "metadata": {
    "id": "8125f85a-b3e6-4468-91b2-f0105f9d3a8f"
   },
   "source": [
    "We are now plotting each cluster to see the distribution of its individual subgenres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9026f-2ebf-4050-b220-0c2420b91bdd",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1668794475531,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "62a9026f-2ebf-4050-b220-0c2420b91bdd"
   },
   "outputs": [],
   "source": [
    "cluster_names = ['Comedy', 'Drama', 'Horror', 'Romance', 'Criminal/Thriller', 'Action/Adventure', 'Entertainment/Hobby', 'Fiction',\\\n",
    "                 'Erotism','Science Fiction', 'Animation', 'Musical', 'Fantasy', 'Western', 'Biography', 'LGBTQ+', 'Apocalyptic',\\\n",
    "                 'Historical', 'Societal','Religious', 'Engaged', 'Ethnic', 'Low budget', 'Current, movement']\n",
    "colors = ['lightcoral', 'forestgreen', 'coral', 'gold', 'indianred', 'silver', 'orange', 'navy', 'yellowgreen',\\\n",
    "          'bisque', 'mediumorchid', 'limegreen', 'darkgoldenrod', 'turquoise', 'deeppink', 'lightblue',\\\n",
    "          'rosybrown','steelblue', 'pink', 'brown', 'khaki', 'gray', 'violet', 'palegreen'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b7cc4-9b9b-4fe7-9aee-49141e4a64d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10929,
     "status": "ok",
     "timestamp": 1668794491293,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "f76b7cc4-9b9b-4fe7-9aee-49141e4a64d6",
    "outputId": "e2a6e2b0-80de-4127-cfec-e06d68d1d145"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(12, 2, figsize = (12,50), dpi=200)\n",
    "fig.tight_layout(h_pad=12, w_pad=2)\n",
    "\n",
    "plot_clusters(0,0, group_comedy, numbers_comedy, cluster_names[0], colors[0])\n",
    "plot_clusters(0,1, group_drama, numbers_drama, cluster_names[1], colors[1])\n",
    "plot_clusters(1,0, group_horror, numbers_horror, cluster_names[2], colors[2])\n",
    "plot_clusters(1,1, group_romance, numbers_romance, cluster_names[3], colors[3])\n",
    "plot_clusters(2,0, group_criminal, numbers_criminal, cluster_names[4], colors[4])\n",
    "plot_clusters(2,1, group_action_adv, numbers_action_adv, cluster_names[5], colors[5])\n",
    "plot_clusters(3,0, group_entert_hobb, numbers_entert_hobb, cluster_names[6], colors[6])\n",
    "plot_clusters(3,1, group_fiction, numbers_fiction, cluster_names[7], colors[7])\n",
    "plot_clusters(4,0, group_erotism, numbers_erotism, cluster_names[8], colors[8])\n",
    "plot_clusters(4,1, group_scifi, numbers_scifi, cluster_names[9], colors[9])\n",
    "plot_clusters(5,0, group_animation, numbers_animation, cluster_names[10], colors[10])\n",
    "plot_clusters(5,1, group_musical, numbers_musical, cluster_names[11], colors[11])\n",
    "plot_clusters(6,0, group_fantasy, numbers_fantasy, cluster_names[12], colors[12])\n",
    "plot_clusters(6,1, group_western, numbers_western, cluster_names[13], colors[13])\n",
    "plot_clusters(7,0, group_biography, numbers_biography, cluster_names[14], colors[14])\n",
    "plot_clusters(7,1, group_queer, numbers_queer, cluster_names[15], colors[15])\n",
    "plot_clusters(8,0, group_apocalypse, numbers_apocalypse, cluster_names[16], colors[16])\n",
    "plot_clusters(8,1, group_history, numbers_hystory, cluster_names[17], colors[17])\n",
    "plot_clusters(9,0, group_society, numbers_society, cluster_names[18], colors[18])\n",
    "plot_clusters(9,1, group_religion, numbers_religion, cluster_names[19], colors[19])\n",
    "plot_clusters(10,0, group_engaged, numbers_engaged, cluster_names[20], colors[20])\n",
    "plot_clusters(10,1, group_world, numbers_world, cluster_names[21], colors[21])\n",
    "plot_clusters(11,0, group_lowbud, numbers_lowbud, cluster_names[22], colors[22])\n",
    "plot_clusters(11,1, group_ism, numbers_ism, cluster_names[23], colors[23])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40a254-e690-41e6-ac0a-3698d32de993",
   "metadata": {
    "id": "6e40a254-e690-41e6-ac0a-3698d32de993"
   },
   "source": [
    "Now we visualized which genres were present in each cluster, we visualize the distribution of these clusters across movies. Again, these clusters may not be fully representative of the entire movie set. But it is sufficient for a broad analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26543dbd-c49d-4465-a202-82d33137a5ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "executionInfo": {
     "elapsed": 418,
     "status": "error",
     "timestamp": 1668794764226,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "26543dbd-c49d-4465-a202-82d33137a5ef",
    "outputId": "d31e53a8-228c-44ed-b485-de6533ef3545"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6), dpi=100)\n",
    "cluster_counts = [sum(numbers_comedy), sum(numbers_drama), sum(numbers_horror), sum(numbers_romance), sum(numbers_criminal),\\\n",
    "                  sum(numbers_action_adv), sum(numbers_entert_hobb), sum(numbers_fiction), sum(numbers_erotism), sum(numbers_scifi),\\\n",
    "                  sum(numbers_animation), sum(numbers_musical), sum(numbers_fantasy), sum(numbers_western), sum(numbers_biography),\\\n",
    "                  sum(numbers_queer), sum(numbers_apocalypse), sum(numbers_hystory), sum(numbers_society), sum(numbers_religion),\\\n",
    "                  sum(numbers_engaged), sum(numbers_world), sum(numbers_lowbud), sum(numbers_ism)]\n",
    "plt.bar(cluster_names, cluster_counts, color=colors) \n",
    "plt.tick_params(labelrotation=90)\n",
    "plt.ylabel('counts')\n",
    "plt.title('Distribution of manually-determined clusters across movies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc26a0",
   "metadata": {
    "id": "71bc26a0",
    "tags": []
   },
   "source": [
    "### 2.3.4 Words polarity <a id='2.3.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff56c9-10e6-4f7b-a8b6-34b945923231",
   "metadata": {
    "id": "9fff56c9-10e6-4f7b-a8b6-34b945923231"
   },
   "source": [
    "Here we want to create 2 additional columns in `movies` indicating the count of positive and negative terms present in plot summaries. We make an intersection between each plot words list and each list of positive or negative words. The positive and negative lists of words are created from online text documents using the get function, the missing values and the title are filtered to get the relevant terms only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lZ6VwtEv6tST",
   "metadata": {
    "executionInfo": {
     "elapsed": 1768,
     "status": "ok",
     "timestamp": 1668794606911,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "lZ6VwtEv6tST"
   },
   "outputs": [],
   "source": [
    "# Count of the different negative and positive words\n",
    "positive_txt = requests.get('https://ptrckprry.com/course/ssd/data/positive-words.txt').text\n",
    "negative_txt = requests.get('https://ptrckprry.com/course/ssd/data/negative-words.txt').text\n",
    "\n",
    "def parse_str(s):\n",
    "    return list(filter(lambda x:x[0]!=';', list(filter(None, s.split(\"\\n\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PoUdjXXW6vaF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 107560,
     "status": "ok",
     "timestamp": 1668794714443,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "PoUdjXXW6vaF",
    "outputId": "719429bc-4642-485f-8ce7-4f89c9d5f89e"
   },
   "outputs": [],
   "source": [
    "movies['plot_num_positive']= movies.Plot.apply(lambda x : len(set(parse_str(positive_txt)) & set(RegexpTokenizer(r'\\w+').tokenize(x.lower()))))\n",
    "movies['plot_num_negative']= movies.Plot.apply(lambda x : len(set(parse_str(negative_txt)) & set(RegexpTokenizer(r'\\w+').tokenize(x.lower()))))\n",
    "\n",
    "movies[['plot_num_positive','plot_num_negative']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e67a64-8d74-41bb-b359-dc4164f3a2fe",
   "metadata": {
    "id": "97e67a64-8d74-41bb-b359-dc4164f3a2fe"
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zIwkHGr33279",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1668794718509,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "zIwkHGr33279",
    "outputId": "1f84b695-5c2e-4cd1-fd2f-ac4d5c3de656"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14,5))\n",
    "fig.tight_layout(pad=2)\n",
    "axs[0].hist(movies.plot_num_negative,bins=40,log=True)\n",
    "axs[0].set_title('Distribution of negative words across movies')\n",
    "axs[0].set_xlabel('Number negative words')\n",
    "axs[0].set_ylabel('Counts (log)')\n",
    "\n",
    "axs[1].hist(movies.plot_num_positive,bins=40,log=True)\n",
    "axs[1].set_title('Distribution of positive words across movies')\n",
    "axs[1].set_xlabel('Number of positive words')\n",
    "axs[1].set_ylabel('Counts (log)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LHKfEubH4CZU",
   "metadata": {
    "id": "LHKfEubH4CZU"
   },
   "source": [
    "We observe that most movies use mainly a relatively small number of positive and negative words. Indeed, the distribution is the highest between 0 and 10 words. The main part use 40 or less positive words and 80 or less negative words.  \n",
    "\n",
    "A deeper anaysis could be done across genres or countries instead of movies. We will let this for Milestone3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltUL4Wnt4Lia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1668794719123,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "ltUL4Wnt4Lia",
    "outputId": "1e7601f6-25e6-409f-d77e-8ff36e46051f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.scatter(movies.plot_num_negative, movies.plot_num_positive, s=10)\n",
    "plt.title('Relationship between number of negative words and number of \\npositive words in plot summaries')\n",
    "plt.xlabel('Number of negative words')\n",
    "plt.ylabel('Number of positive words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mN2mlik44RnM",
   "metadata": {
    "id": "mN2mlik44RnM"
   },
   "source": [
    "We observe an approximative linear trend : few negative words used correspond to few positive words used, and a lot of negative words correspond to a lot of positive words. However, most movies use a small/an average number of both positive and negative words.  \n",
    "\n",
    "Again, this analysis will be further explored with genres and countries in Milestone3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa529fd-59e1-434e-84ce-c85d0f5ccf23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1668794719125,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "baa529fd-59e1-434e-84ce-c85d0f5ccf23",
    "outputId": "43e23743-1298-4ead-dedd-ae0e1511fbbc"
   },
   "outputs": [],
   "source": [
    "print('Linear association between count of negative words and count of positive words across plot summaries:')\n",
    "print(stats.pearsonr(movies.plot_num_negative, movies.plot_num_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1b3c9-e167-418d-9ff2-74a08ffee2cc",
   "metadata": {
    "id": "c5e1b3c9-e167-418d-9ff2-74a08ffee2cc"
   },
   "source": [
    "Since we have a Pearson's r value of 0.72, there is a relatively strong significant linear correlation between number of negative and positive words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4931d-c9c5-4216-a08d-5e021fc366ed",
   "metadata": {
    "id": "83e4931d-c9c5-4216-a08d-5e021fc366ed"
   },
   "source": [
    "### 2.3.5 Metascore preliminary visualization <a id='2.3.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D5HIeOUP7wNi",
   "metadata": {
    "id": "D5HIeOUP7wNi"
   },
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1SLDCrGo7hEt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1668794972752,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "1SLDCrGo7hEt",
    "outputId": "b70a1c57-02a2-4797-8d0e-cc76e78d5cab"
   },
   "outputs": [],
   "source": [
    "movies.metascore.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97c008-1db8-4178-978b-fa60d43f2b8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 1307,
     "status": "ok",
     "timestamp": 1668794881487,
     "user": {
      "displayName": "Salomé Baup",
      "userId": "07571536397946550023"
     },
     "user_tz": -60
    },
    "id": "9a97c008-1db8-4178-978b-fa60d43f2b8d",
    "outputId": "a19eb991-371b-4858-89b6-f08b19286534"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "movies.metascore.hist(bins=100)\n",
    "plt.title('Distribution of metascores across all movies')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JGwDTEb07zMR",
   "metadata": {
    "id": "JGwDTEb07zMR"
   },
   "source": [
    "The metascores distribution looks like a Gaussian distribution. We see some score are never given to movies, at regular intervals!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1323a08-dfb9-4631-9165-01cd46a09e1f",
   "metadata": {
    "id": "f1323a08-dfb9-4631-9165-01cd46a09e1f"
   },
   "source": [
    "### 2.3.6 WordCloud <a id='2.3.6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe1fc1-a9de-4226-897c-1dc9071e5288",
   "metadata": {
    "id": "c1fe1fc1-a9de-4226-897c-1dc9071e5288",
    "outputId": "ebe99ebb-af87-4eb3-d55c-5b69af7dc74c"
   },
   "outputs": [],
   "source": [
    "docs = plot_summaries.Plot.tolist()\n",
    "d = \" \".join(docs)\n",
    "\n",
    "clap = np.array(Image.open(\"data/clap.png\"))\n",
    "\n",
    "# Create a word cloud image\n",
    "wc = WordCloud(background_color=\"white\", max_words=500, mask=clap,\n",
    "               stopwords=STOPWORDS, contour_width=3, contour_color='black', colormap=\"Greys\")\n",
    "\n",
    "# Generate a wordcloud\n",
    "wc.generate(d)\n",
    "\n",
    "# store to file\n",
    "wc.to_file(\"data/clap_wc.png\")\n",
    "\n",
    "# show\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DyLEEnCS6eg_",
   "metadata": {
    "id": "DyLEEnCS6eg_",
    "tags": []
   },
   "source": [
    "#### Saving augmented `movies` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5lCTHA_j6ehD",
   "metadata": {
    "id": "5lCTHA_j6ehD"
   },
   "outputs": [],
   "source": [
    "movies.to_pickle('data/movies_aug_withMetascores.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BufR9Pfz6ehF",
   "metadata": {
    "id": "BufR9Pfz6ehF"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_pickle('data/movies_aug_withMetascores.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d7665",
   "metadata": {
    "id": "9a9d7665"
   },
   "source": [
    "# 3. Topic Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe32486-e1c5-48f6-a455-2f556dafaecb",
   "metadata": {
    "id": "fbe32486-e1c5-48f6-a455-2f556dafaecb",
    "tags": []
   },
   "source": [
    "## 3.1 LDA <a id='3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941481d-6e9a-4119-97c5-6e84a003a0cc",
   "metadata": {
    "id": "4941481d-6e9a-4119-97c5-6e84a003a0cc",
    "tags": []
   },
   "source": [
    "### 3.1.1 Data preparation <a id='3.1.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57772619-9cda-494b-892d-c6d0291bbcb1",
   "metadata": {
    "id": "57772619-9cda-494b-892d-c6d0291bbcb1"
   },
   "source": [
    "#### Prepare bi-grams and tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db24f3",
   "metadata": {
    "id": "08db24f3"
   },
   "outputs": [],
   "source": [
    "tokens = plot_summaries['tokens'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a2d6ad-0368-47ba-ad3b-97e54c982276",
   "metadata": {
    "id": "77a2d6ad-0368-47ba-ad3b-97e54c982276"
   },
   "source": [
    "#### Prepare objects for LDA gensim implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db40c2",
   "metadata": {
    "id": "90db40c2"
   },
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary(tokens)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d0268-cb46-43aa-90e5-d897bddb90a1",
   "metadata": {
    "id": "8b4d0268-cb46-43aa-90e5-d897bddb90a1",
    "tags": []
   },
   "source": [
    "### 3.1.2 Implementation <a id='3.1.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf63e6",
   "metadata": {
    "id": "0abf63e6",
    "outputId": "dea96a6f-dc05-416a-d75c-047bf1e365f9"
   },
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "num_topics = 20\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae39a3d-1784-4202-99a8-9b03afa3ea39",
   "metadata": {
    "id": "6ae39a3d-1784-4202-99a8-9b03afa3ea39",
    "tags": []
   },
   "source": [
    "### 3.1.3 Model evaluation <a id='3.1.3'></a>\n",
    "\n",
    "(by checking how many topics a word exists in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381afe7d",
   "metadata": {
    "id": "381afe7d"
   },
   "outputs": [],
   "source": [
    "topics_ = dict()\n",
    "for i,topic in lda_model.show_topics(formatted=False, num_topics=num_topics, num_words=20):\n",
    "    topics_[i]=topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6e552",
   "metadata": {
    "id": "3be6e552"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Counts for each word the number of topics that include it\n",
    "\"\"\"\n",
    "def count_words(topics_):\n",
    "    counts = dict()\n",
    "    for i in topics_:\n",
    "        for word in topics_[i]:\n",
    "            if word[0] in counts:\n",
    "                counts[word[0]]+=1\n",
    "            else:\n",
    "                counts[word[0]]=1\n",
    "    return counts\n",
    "\n",
    "def takeSecond(elem):\n",
    "    return elem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8759e8f",
   "metadata": {
    "id": "a8759e8f",
    "outputId": "67c706a2-9d8d-4a00-83ce-4ba2c28f6869"
   },
   "outputs": [],
   "source": [
    "counts = count_words(topics_)\n",
    "repeated_words = [(k,v) for k,v in counts.items() if v>=4]\n",
    "repeated_words.sort(reverse=True, key = takeSecond)\n",
    "repeated_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759452a2-94d7-467f-a7ad-2ddf474740e2",
   "metadata": {
    "id": "759452a2-94d7-467f-a7ad-2ddf474740e2",
    "tags": []
   },
   "source": [
    "### 3.1.4 Resulting topics <a id='3.1.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc77619",
   "metadata": {
    "id": "ddc77619",
    "outputId": "3a96553a-d8ae-4b1e-e2e7-07c5a2859c1a"
   },
   "outputs": [],
   "source": [
    "for i in topics_:\n",
    "    message = \"Topic {} : \".format(i) \n",
    "    words = str([k for (k,v) in topics_[i]])\n",
    "    print(message+words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639dccf1-c908-4333-b68f-9a7f75204194",
   "metadata": {
    "id": "639dccf1-c908-4333-b68f-9a7f75204194",
    "tags": []
   },
   "source": [
    "### 3.1.5 Topic Visualizations <a id='3.1.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989be7e6-470f-42d2-9d12-924de5230fc8",
   "metadata": {},
   "source": [
    "Unfortunately, since the plot is interactive when we push the notebook to github it disappears. Hopefully, by milestone 3 we can find a way to host it in a github page. In the meantime, you can find a [screenshot](https://github.com/epfl-ada/ada-2022-project-adaccord/blob/main/topic_visualiation.png) of it in our root repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2afa5e",
   "metadata": {
    "id": "7b2afa5e"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "vis = gensimvis.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary_LDA)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe4e3d-3e8a-41ba-b2cf-c232e3bc2cee",
   "metadata": {
    "id": "2bfe4e3d-3e8a-41ba-b2cf-c232e3bc2cee",
    "tags": []
   },
   "source": [
    "### 3.1.6 Assigning topics to movies <a id='3.1.6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb28a9d",
   "metadata": {
    "id": "dfb28a9d"
   },
   "outputs": [],
   "source": [
    "def most_probable_topic(possible_topics):\n",
    "    possible_topics.sort(reverse=True, key = takeSecond)\n",
    "    return possible_topics[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b101b",
   "metadata": {
    "id": "131b101b",
    "outputId": "03f40bc0-7acc-4073-f66a-d30498b6ace5"
   },
   "outputs": [],
   "source": [
    "chosen_topics = list()\n",
    "for plot in tqdm(corpus):\n",
    "    possible_topics = lda_model[plot]\n",
    "    chosen_topic = most_probable_topic(possible_topics)\n",
    "    chosen_topics.append(chosen_topic)\n",
    "\n",
    "chosen_topics = np.array(chosen_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5de8d",
   "metadata": {
    "id": "d4d5de8d",
    "outputId": "1c331bc8-5d71-4e95-8041-33db0840f384"
   },
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(chosen_topics, columns = [\"Topic\"]) #Rename this to Topic_LDA\n",
    "\n",
    "topics_df[\"Index\"] = topics_df.index\n",
    "\n",
    "display(topics_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd6755",
   "metadata": {
    "id": "6dfd6755"
   },
   "outputs": [],
   "source": [
    "plot_summaries[\"Index\"] = plot_summaries.index\n",
    "\n",
    "plot_summaries = plot_summaries.merge(topics_df, left_on=\"Index\", right_on=\"Index\").drop(columns=[\"Index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bd929",
   "metadata": {
    "id": "276bd929",
    "outputId": "53ed7884-71c4-4707-8a9f-e7376cfb2809"
   },
   "outputs": [],
   "source": [
    "plot_summaries[[\"WikiMovieID\", \"Plot\", \"Topic\"]].head() # Can merge again with movies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdf064-7568-4af8-b1d5-ee75f090925e",
   "metadata": {
    "id": "68cdf064-7568-4af8-b1d5-ee75f090925e"
   },
   "source": [
    "#### Saving augmented `movies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ea89d",
   "metadata": {
    "id": "971ea89d"
   },
   "outputs": [],
   "source": [
    "merging_cols = list(plot_summaries.columns.difference(movies.columns)) + [\"WikiMovieID\"]\n",
    "movies = movies.merge(plot_summaries[merging_cols], how=\"left\", on=\"WikiMovieID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95a9a1",
   "metadata": {
    "id": "ae95a9a1"
   },
   "outputs": [],
   "source": [
    "os.makedirs('data', exist_ok=True)  \n",
    "movies.to_pickle('data/movies_aug_lda.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84175ef1",
   "metadata": {
    "id": "84175ef1"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_pickle('data/movies_aug_lda.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0eb59",
   "metadata": {
    "id": "c8a0eb59",
    "tags": []
   },
   "source": [
    "## 3.2 BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af8858-6530-4bd1-9354-928a1eece5a1",
   "metadata": {
    "id": "08af8858-6530-4bd1-9354-928a1eece5a1"
   },
   "source": [
    "[BERTopic](https://maartengr.github.io/BERTopic/index.html) is a topic modeling technique that leverages 🤗 transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. We will use this model to extract once more topics for each movie plots. The reason we repeat this process twice is so we can compare the results between them and by doing so verify them. In the end we will hopefully find a way to merge our results into a more concrete topic prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94d81b-baff-4f2b-a959-bfdb73f8f38e",
   "metadata": {
    "id": "5a94d81b-baff-4f2b-a959-bfdb73f8f38e",
    "tags": []
   },
   "source": [
    "### 3.2.1 Data preparation <a id='3.2.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991a1b3-04e8-43be-89b2-88b13fe25b97",
   "metadata": {
    "id": "d991a1b3-04e8-43be-89b2-88b13fe25b97"
   },
   "source": [
    "BERTopic requires different preparation than LDA. We begin with creating a list of documents where each document is the plot for a movie. Then, we tokenize each document and remove stopwords, punctuations and the names found in [2.1.6](#2.1.6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9026fb",
   "metadata": {
    "id": "be9026fb",
    "outputId": "6265b236-d58c-42e5-9534-ab56dd1ef3bf"
   },
   "outputs": [],
   "source": [
    "# List of docs\n",
    "docs = movies[~movies.Plot.isna()].Plot.tolist()\n",
    "\n",
    "# List of punctuation marks\n",
    "punctuations = list()\n",
    "for punctuation in string.punctuation:\n",
    "    punctuations.append(punctuation)\n",
    "\n",
    "# Stopwords to use\n",
    "stop_words = list(set(stopwords.words('english'))) + punctuations + names\n",
    "\n",
    "# Tokenization : List of Lists of Tokens\n",
    "docs_tokenized = [word_tokenize(doc) for doc in docs]\n",
    "\n",
    "# Removing the stopwords\n",
    "filtered_docs = list()\n",
    "for doc in tqdm(docs_tokenized):\n",
    "    filtered_doc = \" \".join([w.lower() for w in doc if not w.lower() in stop_words])\n",
    "    filtered_docs.append(filtered_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c36fd-c598-44a7-bba3-24b03c956f30",
   "metadata": {
    "id": "c55c36fd-c598-44a7-bba3-24b03c956f30",
    "tags": []
   },
   "source": [
    "### 3.2.2 Implementation <a id='3.2.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0036ff-33d4-4911-a4cb-49a38d33e1d5",
   "metadata": {
    "id": "7f0036ff-33d4-4911-a4cb-49a38d33e1d5"
   },
   "source": [
    "BERT is a transformer-based machine learning technique which is unfortunately computationally intensive. Therefore, we decided to execute it in google colab to utilise the provided gpu. The instructions followed there are the same as the ones portrayed in this notebook. We then saved the model and its predictions. The model can be found in this [drive](https://drive.google.com/drive/folders/1pUw3DCFzGdlNXRTiX8NZgG0wjSsQYbkZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae32ef",
   "metadata": {
    "id": "b3ae32ef"
   },
   "outputs": [],
   "source": [
    "#model = BERTopic(language=\"english\")\n",
    "#topics, probs = model.fit_transform(filtered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb17bc-e40b-476f-a1f0-271c667cd2e9",
   "metadata": {
    "id": "5ddb17bc-e40b-476f-a1f0-271c667cd2e9",
    "tags": []
   },
   "source": [
    "### 3.2.3 Model Evaluation <a id='3.2.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104c8e1-47b0-4a45-903b-ca9de892d1ed",
   "metadata": {
    "id": "3104c8e1-47b0-4a45-903b-ca9de892d1ed"
   },
   "source": [
    "We saved our model form cloud using `pickle` and the equivalent command given by the `bertopic` [library](https://maartengr.github.io/BERTopic/index.html#common). However when we try to load them we get the following error message : *RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU*. For the moment, we can't solve this problem so we will import our results directly. The commands used in google colab to get these results are provided too but they are commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2992901-ec2a-4f1e-9328-89b138c63a59",
   "metadata": {
    "id": "a2992901-ec2a-4f1e-9328-89b138c63a59"
   },
   "outputs": [],
   "source": [
    "#model = BERTopic.load('models/BERTopic')\n",
    "#model = pickle.load(open('models/bert_model', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c84d5-2e7b-4801-8b79-9702bbbb3c41",
   "metadata": {
    "id": "c23c84d5-2e7b-4801-8b79-9702bbbb3c41",
    "tags": []
   },
   "source": [
    "### 3.2.4 Predictions <a id='3.2.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67b5ce-6710-45d5-a5b7-b5e37e9c31d3",
   "metadata": {
    "id": "2f67b5ce-6710-45d5-a5b7-b5e37e9c31d3"
   },
   "outputs": [],
   "source": [
    "bert_predictions = pd.read_pickle('predictions/bert_predictions.pkl')\n",
    "\n",
    "bert_predictions.rename(columns={\"Topic\":\"BERT\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d6ff3-3ca7-4cdb-9ca0-228a1e446e4f",
   "metadata": {
    "id": "419d6ff3-3ca7-4cdb-9ca0-228a1e446e4f",
    "tags": []
   },
   "source": [
    "### 3.2.5 Topic Visualizations <a id='3.2.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23dd4a2-0753-4b87-a79d-a41af1d87af1",
   "metadata": {
    "id": "f23dd4a2-0753-4b87-a79d-a41af1d87af1"
   },
   "source": [
    "Labels for the top 10 most common topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5603db8c-cb97-4577-ad5a-a6e44cd0414e",
   "metadata": {
    "id": "5603db8c-cb97-4577-ad5a-a6e44cd0414e",
    "outputId": "136cb982-a6a9-492a-dadb-659c20ab8eca"
   },
   "outputs": [],
   "source": [
    "#bert_topics = model.generate_topic_labels()\n",
    "\n",
    "bert_topics = pd.read_pickle('predictions/bert_topics.pkl')\n",
    "bert_topics = pd.DataFrame(bert_topics, columns = [\"Bert_Topic\"])\n",
    "bert_topics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d8016-4d65-42b1-9478-f1805a265ee1",
   "metadata": {
    "id": "e55d8016-4d65-42b1-9478-f1805a265ee1",
    "tags": []
   },
   "source": [
    "### 3.2.6 Assigning topics to movies <a id='3.2.6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56d0c1-efdb-48af-b831-aee999deade8",
   "metadata": {
    "id": "2e56d0c1-efdb-48af-b831-aee999deade8",
    "outputId": "09c8f404-ef98-4e5e-a723-05ca280b6878"
   },
   "outputs": [],
   "source": [
    "movies = movies.merge(bert_predictions, how=\"left\", on=\"WikiMovieID\")\n",
    "movies[[\"Plot\", \"BERT\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d3887-96ed-4c15-9546-278c6adf269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data', exist_ok=True)  \n",
    "movies.to_pickle('data/movies_aug_bert.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0239b6-8360-41bc-8357-c56761b8131c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbac97-880c-4c13-b836-952f3c0680d4",
   "metadata": {},
   "source": [
    "# Removing names using POS-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f82192-4991-4c45-86ba-862f8c403f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_pickle('data/movies_aug_bert.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "92ff7735-af14-46ae-8804-42648555a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 42174/42174 [00:01<00:00, 36699.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>POS_tokens</th>\n",
       "      <th>names</th>\n",
       "      <th>tokens_no_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[set, second, half, century, film, depict, mar...</td>\n",
       "      <td>[[(Set, NN), (in, IN), (the, DT), (second, JJ)...</td>\n",
       "      <td>[mars, melanie, ballard, sergeant, jericho, de...</td>\n",
       "      <td>[set, second, half, century, film, depict, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[series, murder, woman, throughout, arizona, d...</td>\n",
       "      <td>[[(A, DT), (series, NN), (of, IN), (murders, N...</td>\n",
       "      <td>[arizona, detective, charles, mendoza, paul, w...</td>\n",
       "      <td>[series, murder, woman, throughout, distinctiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[upper, class, housewife, becomes, frustrated,...</td>\n",
       "      <td>[[(Eva, NNP), (,, ,), (an, DT), (upper, JJ), (...</td>\n",
       "      <td>[eva, yvonne, eva, johns, eva, eva, chris, chris]</td>\n",
       "      <td>[upper, class, housewife, becomes, frustrated,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[every, hundred, year, evil, morgana, return, ...</td>\n",
       "      <td>[[(Every, DT), (hundred, CD), (years, NNS), (,...</td>\n",
       "      <td>[morgana, fingall, merlin, young, ben, clark, ...</td>\n",
       "      <td>[every, hundred, year, evil, return, claim, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[artist, work, cab, driver, side, hard, time, ...</td>\n",
       "      <td>[[(Adam, NNP), (,, ,), (a, DT), (San, NNP), (F...</td>\n",
       "      <td>[adam, san, nina, kate, nina, kate, nina, adam...</td>\n",
       "      <td>[artist, work, cab, driver, side, hard, time, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42169</th>\n",
       "      <td>[plot, film, open, great, western, express, sp...</td>\n",
       "      <td>[[({, (), ({, (), (plot, NN), (}, )), (}, )), ...</td>\n",
       "      <td>[box, tunnel, cornwall, herbert, edna, truro, ...</td>\n",
       "      <td>[plot, film, open, great, western, speed, rout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42170</th>\n",
       "      <td>[former, national, oceanic, atmospheric, admin...</td>\n",
       "      <td>[[(Two, CD), (former, JJ), (National, NNP), (O...</td>\n",
       "      <td>[national, oceanic, atmospheric, administration]</td>\n",
       "      <td>[former, scientist, investigate, mass, strandi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42171</th>\n",
       "      <td>[plot, film, follow, year, life, irish, travel...</td>\n",
       "      <td>[[({, (), ({, (), (No, DT), (plot, NN), (}, ))...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[plot, film, follow, year, life, irish, travel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42172</th>\n",
       "      <td>[story, place, year, super, dimension, fortres...</td>\n",
       "      <td>[[(The, DT), (story, NN), (takes, VBZ), (place...</td>\n",
       "      <td>[super, dimension, fortress, macross, ii, orig...</td>\n",
       "      <td>[story, place, year, cd, booklet, eighty, year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42173</th>\n",
       "      <td>[movie, teenage, girl, horror, movie, watch, t...</td>\n",
       "      <td>[[(The, DT), (movie, NN), (is, VBZ), (about, I...</td>\n",
       "      <td>[wisher]</td>\n",
       "      <td>[movie, teenage, girl, horror, movie, watch, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42174 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  \\\n",
       "0      [set, second, half, century, film, depict, mar...   \n",
       "1      [series, murder, woman, throughout, arizona, d...   \n",
       "2      [upper, class, housewife, becomes, frustrated,...   \n",
       "3      [every, hundred, year, evil, morgana, return, ...   \n",
       "4      [artist, work, cab, driver, side, hard, time, ...   \n",
       "...                                                  ...   \n",
       "42169  [plot, film, open, great, western, express, sp...   \n",
       "42170  [former, national, oceanic, atmospheric, admin...   \n",
       "42171  [plot, film, follow, year, life, irish, travel...   \n",
       "42172  [story, place, year, super, dimension, fortres...   \n",
       "42173  [movie, teenage, girl, horror, movie, watch, t...   \n",
       "\n",
       "                                              POS_tokens  \\\n",
       "0      [[(Set, NN), (in, IN), (the, DT), (second, JJ)...   \n",
       "1      [[(A, DT), (series, NN), (of, IN), (murders, N...   \n",
       "2      [[(Eva, NNP), (,, ,), (an, DT), (upper, JJ), (...   \n",
       "3      [[(Every, DT), (hundred, CD), (years, NNS), (,...   \n",
       "4      [[(Adam, NNP), (,, ,), (a, DT), (San, NNP), (F...   \n",
       "...                                                  ...   \n",
       "42169  [[({, (), ({, (), (plot, NN), (}, )), (}, )), ...   \n",
       "42170  [[(Two, CD), (former, JJ), (National, NNP), (O...   \n",
       "42171  [[({, (), ({, (), (No, DT), (plot, NN), (}, ))...   \n",
       "42172  [[(The, DT), (story, NN), (takes, VBZ), (place...   \n",
       "42173  [[(The, DT), (movie, NN), (is, VBZ), (about, I...   \n",
       "\n",
       "                                                   names  \\\n",
       "0      [mars, melanie, ballard, sergeant, jericho, de...   \n",
       "1      [arizona, detective, charles, mendoza, paul, w...   \n",
       "2      [eva, yvonne, eva, johns, eva, eva, chris, chris]   \n",
       "3      [morgana, fingall, merlin, young, ben, clark, ...   \n",
       "4      [adam, san, nina, kate, nina, kate, nina, adam...   \n",
       "...                                                  ...   \n",
       "42169  [box, tunnel, cornwall, herbert, edna, truro, ...   \n",
       "42170   [national, oceanic, atmospheric, administration]   \n",
       "42171                                                 []   \n",
       "42172  [super, dimension, fortress, macross, ii, orig...   \n",
       "42173                                           [wisher]   \n",
       "\n",
       "                                         tokens_no_names  \n",
       "0      [set, second, half, century, film, depict, pla...  \n",
       "1      [series, murder, woman, throughout, distinctiv...  \n",
       "2      [upper, class, housewife, becomes, frustrated,...  \n",
       "3      [every, hundred, year, evil, return, claim, ta...  \n",
       "4      [artist, work, cab, driver, side, hard, time, ...  \n",
       "...                                                  ...  \n",
       "42169  [plot, film, open, great, western, speed, rout...  \n",
       "42170  [former, scientist, investigate, mass, strandi...  \n",
       "42171  [plot, film, follow, year, life, irish, travel...  \n",
       "42172  [story, place, year, cd, booklet, eighty, year...  \n",
       "42173  [movie, teenage, girl, horror, movie, watch, t...  \n",
       "\n",
       "[42174 rows x 4 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_names(pos_tags):\n",
    "    names = []\n",
    "    for sentence in pos_tags:\n",
    "        names.extend([word.lower() for (word, pos) in sentence if pos =='NNP'])\n",
    "    \n",
    "    return names\n",
    "\n",
    "movies[\"names\"] = movies.POS_tokens.progress_apply(find_names)\n",
    "movies[\"tokens_no_names\"] = [list(filter(lambda x : x not in names, tokens)) for names, tokens in zip(movies.names, movies.tokens)]\n",
    "\n",
    "\n",
    "movies[[\"tokens\", \"POS_tokens\", \"names\", \"tokens_no_names\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ed90dd64-7f4e-4acc-9568-2eeaf94541a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daily', 'planet', 'superman', 'thanks'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use this to check the differences (filtering done) for each row : \n",
    "row = 7\n",
    "set(movies.tokens[row])-set(movies.tokens_no_names[row])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "c8a0eb59"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
